{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0I-IEE0ohyt"
      },
      "source": [
        "### Настройка Airflow\n",
        "\n",
        "Для начала вам необходимо выполнить ряд команд чтобы настроить окружение для дальнейшей работы, это позволит первое время не заниматься настройкой среды исполнения, а сразу начать писать код и работать с Airflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0unBvZMyx5H-",
        "outputId": "5be082e3-9c35-48af-d4ae-cc2af94cc14a"
      },
      "outputs": [],
      "source": [
        "# Установка Airflow\n",
        "!pip install apache-airflow==2.1.4\n",
        "!pip install wtforms==2.3.3\n",
        "!pip install airflow.providers.http\n",
        "\n",
        "# Инициализация базы данных\n",
        "!airflow db init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ri_LIxa4z08a",
        "outputId": "29efcf9a-d3f4-45e9-a545-20f90a32e98f"
      },
      "outputs": [],
      "source": [
        "# Создадим необходимые папки\n",
        "!mkdir /root/airflow/dags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qx1Jfp2O0CBP",
        "outputId": "36df6755-c04d-4bef-b5a2-1714223f078a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;33m/usr/local/lib/python3.10/dist-packages/flask_appbuilder/models/sqla/\u001b[0m\u001b[1;33minterface.py\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m68\u001b[0m\u001b[1;33m SAWarning\u001b[0m\u001b[33m: relationship \u001b[0m\u001b[33m'DagRun.serialized_dag'\u001b[0m\u001b[33m will copy column serialized_dag.dag_id to column dag_run.dag_id, which conflicts with \u001b[0m\u001b[1;33mrelationship\u001b[0m\u001b[1;33m(\u001b[0m\u001b[33ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[33m'DagRun.task_instances'\u001b[0m\u001b[33m \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mcopies task_instance.dag_id to dag_run.dag_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m, \u001b[0m\u001b[33m'TaskInstance.dag_run'\u001b[0m\u001b[33m \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mcopies task_instance.dag_id to dag_run.dag_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m. If this is not the intention, consider if these relationships should be linked with back_populates, or if \u001b[0m\u001b[33mviewonly\u001b[0m\u001b[33m=\u001b[0m\u001b[3;33mTrue\u001b[0m\u001b[33m should be applied to one or more if they are read-only. For the less common case that foreign key constraints are partially overlapping, the \u001b[0m\u001b[1;33morm.foreign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m annotation can be used to isolate the columns that should be written towards.   To silence this warning, add the parameter \u001b[0m\u001b[33m'\u001b[0m\u001b[33moverlaps\u001b[0m\u001b[33m=\u001b[0m\u001b[33m\"dag_run\u001b[0m\u001b[33m,task_instances\"'\u001b[0m\u001b[33m to the \u001b[0m\u001b[33m'DagRun.serialized_dag'\u001b[0m\u001b[33m relationship. \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mBackground on this error at: \u001b[0m\u001b[4;33mhttps://sqlalche.me/e/14/qzyx\u001b[0m\u001b[4;33m)\u001b[0m\n",
            "\u001b[1;33m/usr/local/lib/python3.10/dist-packages/flask_appbuilder/models/sqla/\u001b[0m\u001b[1;33minterface.py\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m68\u001b[0m\u001b[1;33m SAWarning\u001b[0m\u001b[33m: relationship \u001b[0m\u001b[33m'SerializedDagModel.dag_runs'\u001b[0m\u001b[33m will copy column serialized_dag.dag_id to column dag_run.dag_id, which conflicts with \u001b[0m\u001b[1;33mrelationship\u001b[0m\u001b[1;33m(\u001b[0m\u001b[33ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[33m'DagRun.task_instances'\u001b[0m\u001b[33m \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mcopies task_instance.dag_id to dag_run.dag_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m, \u001b[0m\u001b[33m'TaskInstance.dag_run'\u001b[0m\u001b[33m \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mcopies task_instance.dag_id to dag_run.dag_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m. If this is not the intention, consider if these relationships should be linked with back_populates, or if \u001b[0m\u001b[33mviewonly\u001b[0m\u001b[33m=\u001b[0m\u001b[3;33mTrue\u001b[0m\u001b[33m should be applied to one or more if they are read-only. For the less common case that foreign key constraints are partially overlapping, the \u001b[0m\u001b[1;33morm.foreign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m annotation can be used to isolate the columns that should be written towards.   To silence this warning, add the parameter \u001b[0m\u001b[33m'\u001b[0m\u001b[33moverlaps\u001b[0m\u001b[33m=\u001b[0m\u001b[33m\"dag_run\u001b[0m\u001b[33m,task_instances\"'\u001b[0m\u001b[33m to the \u001b[0m\u001b[33m'SerializedDagModel.dag_runs'\u001b[0m\u001b[33m relationship. \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mBackground on this error at: \u001b[0m\u001b[4;33mhttps://sqlalche.me/e/14/qzyx\u001b[0m\u001b[4;33m)\u001b[0m\n",
            "  ____________       _____________\n",
            " ____    |__( )_________  __/__  /________      __\n",
            "____  /| |_  /__  ___/_  /_ __  /_  __ \\_ | /| / /\n",
            "___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /\n",
            " _/_/  |_/_/  /_/    /_/    /_/  \\____/____/|__/\n",
            "[\u001b[34m2023-06-25 11:32:20,960\u001b[0m] {\u001b[34mdagbag.py:\u001b[0m496} INFO\u001b[0m - Filling up the DagBag from \u001b[01m/dev/null\u001b[22m\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/airflow\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/airflow/__main__.py\", line 40, in main\n",
            "    args.func(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/airflow/cli/cli_parser.py\", line 48, in command\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/airflow/utils/cli.py\", line 92, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/airflow/cli/commands/webserver_command.py\", line 368, in webserver\n",
            "    check_if_pidfile_process_is_running(pid_file=pid_file, process_name=\"webserver\")\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/airflow/utils/process_utils.py\", line 267, in check_if_pidfile_process_is_running\n",
            "    raise AirflowException(f\"The {process_name} is already running under PID {pid}.\")\n",
            "airflow.exceptions.AirflowException: The webserver is already running under PID 1048.\n"
          ]
        }
      ],
      "source": [
        "# Включим веб-сервер\n",
        "!airflow webserver -p 18273 -D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AicRo890Iyp",
        "outputId": "e690aefb-0996-4b76-9d63-10a5785f3a0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;33m/usr/local/lib/python3.10/dist-packages/flask_appbuilder/models/sqla/\u001b[0m\u001b[1;33minterface.py\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m68\u001b[0m\u001b[1;33m SAWarning\u001b[0m\u001b[33m: relationship \u001b[0m\u001b[33m'DagRun.serialized_dag'\u001b[0m\u001b[33m will copy column serialized_dag.dag_id to column dag_run.dag_id, which conflicts with \u001b[0m\u001b[1;33mrelationship\u001b[0m\u001b[1;33m(\u001b[0m\u001b[33ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[33m'DagRun.task_instances'\u001b[0m\u001b[33m \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mcopies task_instance.dag_id to dag_run.dag_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m, \u001b[0m\u001b[33m'TaskInstance.dag_run'\u001b[0m\u001b[33m \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mcopies task_instance.dag_id to dag_run.dag_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m. If this is not the intention, consider if these relationships should be linked with back_populates, or if \u001b[0m\u001b[33mviewonly\u001b[0m\u001b[33m=\u001b[0m\u001b[3;33mTrue\u001b[0m\u001b[33m should be applied to one or more if they are read-only. For the less common case that foreign key constraints are partially overlapping, the \u001b[0m\u001b[1;33morm.foreign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m annotation can be used to isolate the columns that should be written towards.   To silence this warning, add the parameter \u001b[0m\u001b[33m'\u001b[0m\u001b[33moverlaps\u001b[0m\u001b[33m=\u001b[0m\u001b[33m\"dag_run\u001b[0m\u001b[33m,task_instances\"'\u001b[0m\u001b[33m to the \u001b[0m\u001b[33m'DagRun.serialized_dag'\u001b[0m\u001b[33m relationship. \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mBackground on this error at: \u001b[0m\u001b[4;33mhttps://sqlalche.me/e/14/qzyx\u001b[0m\u001b[4;33m)\u001b[0m\n",
            "\u001b[1;33m/usr/local/lib/python3.10/dist-packages/flask_appbuilder/models/sqla/\u001b[0m\u001b[1;33minterface.py\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m68\u001b[0m\u001b[1;33m SAWarning\u001b[0m\u001b[33m: relationship \u001b[0m\u001b[33m'SerializedDagModel.dag_runs'\u001b[0m\u001b[33m will copy column serialized_dag.dag_id to column dag_run.dag_id, which conflicts with \u001b[0m\u001b[1;33mrelationship\u001b[0m\u001b[1;33m(\u001b[0m\u001b[33ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[33m'DagRun.task_instances'\u001b[0m\u001b[33m \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mcopies task_instance.dag_id to dag_run.dag_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m, \u001b[0m\u001b[33m'TaskInstance.dag_run'\u001b[0m\u001b[33m \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mcopies task_instance.dag_id to dag_run.dag_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m. If this is not the intention, consider if these relationships should be linked with back_populates, or if \u001b[0m\u001b[33mviewonly\u001b[0m\u001b[33m=\u001b[0m\u001b[3;33mTrue\u001b[0m\u001b[33m should be applied to one or more if they are read-only. For the less common case that foreign key constraints are partially overlapping, the \u001b[0m\u001b[1;33morm.foreign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m annotation can be used to isolate the columns that should be written towards.   To silence this warning, add the parameter \u001b[0m\u001b[33m'\u001b[0m\u001b[33moverlaps\u001b[0m\u001b[33m=\u001b[0m\u001b[33m\"dag_run\u001b[0m\u001b[33m,task_instances\"'\u001b[0m\u001b[33m to the \u001b[0m\u001b[33m'SerializedDagModel.dag_runs'\u001b[0m\u001b[33m relationship. \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mBackground on this error at: \u001b[0m\u001b[4;33mhttps://sqlalche.me/e/14/qzyx\u001b[0m\u001b[4;33m)\u001b[0m\n",
            "admin already exist in the db\n"
          ]
        }
      ],
      "source": [
        "# Создадим пользователя Airflow\n",
        "!airflow users create \\\n",
        "          --username admin \\\n",
        "          --firstname admin \\\n",
        "          --lastname admin \\\n",
        "          --role Admin \\\n",
        "          --email admin@example.org \\\n",
        "          -p 12345"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXlUN86A3m1o",
        "outputId": "296d82fe-eef7-4be0-eab1-85a5ba7b73de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;33m/usr/local/lib/python3.10/dist-packages/airflow/utils/\u001b[0m\u001b[1;33mcli.py\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m149\u001b[0m\u001b[1;33m SAWarning\u001b[0m\u001b[33m: relationship \u001b[0m\u001b[33m'DagRun.serialized_dag'\u001b[0m\u001b[33m will copy column serialized_dag.dag_id to column dag_run.dag_id, which conflicts with \u001b[0m\u001b[1;33mrelationship\u001b[0m\u001b[1;33m(\u001b[0m\u001b[33ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[33m'DagRun.task_instances'\u001b[0m\u001b[33m \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mcopies task_instance.dag_id to dag_run.dag_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m, \u001b[0m\u001b[33m'TaskInstance.dag_run'\u001b[0m\u001b[33m \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mcopies task_instance.dag_id to dag_run.dag_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m. If this is not the intention, consider if these relationships should be linked with back_populates, or if \u001b[0m\u001b[33mviewonly\u001b[0m\u001b[33m=\u001b[0m\u001b[3;33mTrue\u001b[0m\u001b[33m should be applied to one or more if they are read-only. For the less common case that foreign key constraints are partially overlapping, the \u001b[0m\u001b[1;33morm.foreign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m annotation can be used to isolate the columns that should be written towards.   To silence this warning, add the parameter \u001b[0m\u001b[33m'\u001b[0m\u001b[33moverlaps\u001b[0m\u001b[33m=\u001b[0m\u001b[33m\"dag_run\u001b[0m\u001b[33m,task_instances\"'\u001b[0m\u001b[33m to the \u001b[0m\u001b[33m'DagRun.serialized_dag'\u001b[0m\u001b[33m relationship. \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mBackground on this error at: \u001b[0m\u001b[4;33mhttps://sqlalche.me/e/14/qzyx\u001b[0m\u001b[4;33m)\u001b[0m\n",
            "\u001b[1;33m/usr/local/lib/python3.10/dist-packages/airflow/utils/\u001b[0m\u001b[1;33mcli.py\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m149\u001b[0m\u001b[1;33m SAWarning\u001b[0m\u001b[33m: relationship \u001b[0m\u001b[33m'SerializedDagModel.dag_runs'\u001b[0m\u001b[33m will copy column serialized_dag.dag_id to column dag_run.dag_id, which conflicts with \u001b[0m\u001b[1;33mrelationship\u001b[0m\u001b[1;33m(\u001b[0m\u001b[33ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[33m'DagRun.task_instances'\u001b[0m\u001b[33m \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mcopies task_instance.dag_id to dag_run.dag_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m, \u001b[0m\u001b[33m'TaskInstance.dag_run'\u001b[0m\u001b[33m \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mcopies task_instance.dag_id to dag_run.dag_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m. If this is not the intention, consider if these relationships should be linked with back_populates, or if \u001b[0m\u001b[33mviewonly\u001b[0m\u001b[33m=\u001b[0m\u001b[3;33mTrue\u001b[0m\u001b[33m should be applied to one or more if they are read-only. For the less common case that foreign key constraints are partially overlapping, the \u001b[0m\u001b[1;33morm.foreign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m annotation can be used to isolate the columns that should be written towards.   To silence this warning, add the parameter \u001b[0m\u001b[33m'\u001b[0m\u001b[33moverlaps\u001b[0m\u001b[33m=\u001b[0m\u001b[33m\"dag_run\u001b[0m\u001b[33m,task_instances\"'\u001b[0m\u001b[33m to the \u001b[0m\u001b[33m'SerializedDagModel.dag_runs'\u001b[0m\u001b[33m relationship. \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mBackground on this error at: \u001b[0m\u001b[4;33mhttps://sqlalche.me/e/14/qzyx\u001b[0m\u001b[4;33m)\u001b[0m\n",
            "  ____________       _____________\n",
            " ____    |__( )_________  __/__  /________      __\n",
            "____  /| |_  /__  ___/_  /_ __  /_  __ \\_ | /| / /\n",
            "___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /\n",
            " _/_/  |_/_/  /_/    /_/    /_/  \\____/____/|__/\n"
          ]
        }
      ],
      "source": [
        "# Запуск шедулера\n",
        "!airflow scheduler -D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7jJGmYR3nGP",
        "outputId": "d8082e62-0628-4233-9a84-771e9b30a048"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (6.0.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0)\n",
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n",
            "nohup: redirecting stderr to stdout\n"
          ]
        }
      ],
      "source": [
        "# Последующие команды не имеют отношения к Airflow\n",
        "# Они нужни только для корректной работы веб морды\n",
        "# в среде Google Colab\n",
        "\n",
        "!pip install pyngrok\n",
        "!ngrok authtoken  # найти его можно https://dashboard.ngrok.com/get-started/setup\n",
        "\n",
        "# Эта команда просто отображет веб морду на другой адрес\n",
        "# Его вы можете найти https://dashboard.ngrok.com/endpoints/status\n",
        "# При каждом отключении ссылка будет меняться\n",
        "!nohup ngrok http 18273 > /dev/null &"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6sbOlBAm4fUp"
      },
      "source": [
        "После запуска команды выше, перейдите по адресу в ngrok и подождите  пока появится DAG с именем dag"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yah1mva1eYMK"
      },
      "source": [
        "### Задачи"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFYANPhYDZxe",
        "outputId": "b80f2205-7f43-4242-e880-b695ec854b9f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Task(BashOperator): start_task>"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Используя BashOperator написать команду которая будет чистить папку с логами Airflow.\n",
        "from airflow import DAG\n",
        "from datetime import timedelta\n",
        "from airflow.utils.dates import days_ago\n",
        "from airflow.operators.bash import BashOperator\n",
        "\n",
        "dag = DAG('dag',\n",
        "          schedule_interval=timedelta(days=1),\n",
        "          start_date=days_ago(1))\n",
        "\n",
        "\n",
        "bach_op = BashOperator(\n",
        "    task_id='start_task',\n",
        "    bash_command=\"rm -r /root/airflow/logs\",\n",
        "    dag=dag)\n",
        "\n",
        "bach_op"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MI8kjQYSKvQK",
        "outputId": "7a462e06-57d0-4a04-a480-df0b524010dd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Task(SimpleHttpOperator): get_op>"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Используя SimpleHttpOperator обратиться по адресу\n",
        "# https://www.random.org/integers/?num=1&min=1&max=5&col=1&base=2&format=plain\n",
        "# и записать результат в xcom.\n",
        "# { \"args\": {}, \"headers\": { \"Accept\": \"*/*\", \"Accept-Encoding\": \"gzip, deflate\", \"Host\": \"www.httpbin.org\", \"User-Agent\": \"python-requests/2.27.1\",\n",
        "# \"X-Amzn-Trace-Id\": \"Root=1-648da2af-6e86c1400b02ef3f4800085f\" }, \"origin\": \"34.125.25.165\", \"url\": \"https://www.httpbin.org/get\" }\n",
        "\n",
        "from airflow import DAG\n",
        "from datetime import timedelta\n",
        "from airflow.utils.dates import days_ago\n",
        "from airflow.providers.http.operators.http import SimpleHttpOperator\n",
        "\n",
        "dag = DAG('dag2',\n",
        "          schedule_interval=timedelta(days=1),\n",
        "          start_date=days_ago(1))\n",
        "\n",
        "\n",
        "task1 = SimpleHttpOperator(\n",
        "    task_id=\"get_op\",\n",
        "    http_conn_id=\"http_default\",\n",
        "    method=\"GET\",\n",
        "    endpoint=\"get\",\n",
        "    dag=dag)\n",
        "\n",
        "task1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiJvcoa-srOL",
        "outputId": "df0ac672-9cdd-4ee2-ddc2-8468549c6632"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Task(DummyOperator): task_2>"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Переопределить стандартный оператор Dummy так чтобы он пушил в Xcom случайное число от 0 до 9.\n",
        "# По умолчанию, результат выполнения метода execute попадает в Xcom.\n",
        "from airflow import DAG\n",
        "from datetime import timedelta\n",
        "from airflow.utils.dates import days_ago\n",
        "from airflow.models import BaseOperator\n",
        "from jinja2 import Template\n",
        "\n",
        "\n",
        "class DummyOperator(BaseOperator):\n",
        "    ui_color = '#e8f7e4'\n",
        "    #inherits_from_dummy_operator = True\n",
        "\n",
        "    def __init__(self, **kwargs) -> None:\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def execute(self, context):\n",
        "        t = Template ('{{ range(0, 9) | random }}')\n",
        "        return t.render()\n",
        "\n",
        "\n",
        "dag = DAG('dag3',schedule_interval='@daily', start_date=days_ago(1))\n",
        "t1 = DummyOperator(task_id='task_1', dag=dag)\n",
        "t2 = DummyOperator(task_id='task_2',dag=dag)\n",
        "\n",
        "t1 >> t2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WR44fPcszpor",
        "outputId": "40706e4a-725a-4c0e-80f6-852cd41ddf95"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Task(PythonOperator): task_1>"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Создать руками Connection с именем custom_conn_id\n",
        "# Host: google.com\n",
        "# Login: user\n",
        "# Password: 12345\n",
        "# Сделать даг в котором вы обращаетесь к данному подключению и поочереди достаете Host, Login, Password\n",
        "# После того как вы достанете вам необходимо с помощью кода записать эти 3 значения в формате json в Variables\n",
        "from airflow import DAG\n",
        "from datetime import timedelta\n",
        "from airflow.utils.dates import days_ago\n",
        "from airflow.operators.python import PythonOperator\n",
        "from airflow.hooks.base import BaseHook\n",
        "from airflow.models import Variable\n",
        "\n",
        "dag = DAG('dag4',schedule_interval='@daily', start_date=days_ago(1))\n",
        "\n",
        "def conn_to_json(conn_id):\n",
        "  conn = BaseHook.get_connection(conn_id)\n",
        "  json = {'Password': conn.password,\n",
        "         'Login': conn.login,\n",
        "         'URI': conn.get_uri()\n",
        "         }\n",
        "  Variable.set(conn_id, json)\n",
        "\n",
        "t1 = PythonOperator(task_id='task_1',\n",
        "                    python_callable= conn_to_json,\n",
        "                    op_args = ['custom_conn_id'],\n",
        "                    dag=dag)\n",
        "\n",
        "t1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "4kWohrtc5hpU"
      },
      "outputs": [],
      "source": [
        "# Используя HttpSensor обратиться по адресу\n",
        "# https://www.random.org/integers/?num=1&min=1&max=5&col=1&base=10&format=plain\n",
        "# Если ответ будет равен 5 то вернуть True чтобы сенсор завершился, также добавить параметр окончания действия сенсора 1 минутой\n",
        "import airflow\n",
        "from airflow import DAG\n",
        "from airflow.providers.http.sensors.http import HttpSensor\n",
        "import json\n",
        "\n",
        "dag = DAG('dag5',schedule_interval='@daily', start_date=airflow.utils.dates.days_ago(1),)\n",
        "\n",
        "def response_check(response, task_instance):\n",
        "  if response.json() == 5:\n",
        "    return True\n",
        "\n",
        "sensor = HttpSensor(\n",
        "    task_id='http_sensor',\n",
        "    http_conn_id='http_default',\n",
        "    endpoint='',\n",
        "    response_check=response_check,\n",
        "    poke_interval=10,\n",
        "    timeout=60,\n",
        "    dag=dag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "fXXwZX3lNfVy"
      },
      "outputs": [],
      "source": [
        "# Сгенерировать 5 DAG таким образом чтобы в каждом DAG генерировалось по 10 Task идущих параллельно. Использовать DummyOperator для задач.\n",
        "# Имена DAG выбрать по такому шаблону dag_number.\n",
        "from airflow import DAG\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "from airflow.utils.dates import days_ago\n",
        "from airflow.operators.dummy import DummyOperator\n",
        "\n",
        "def create_dag(dag_id,\n",
        "               dag_number,\n",
        "               default_args,\n",
        "               schedule='@daily'):\n",
        "\n",
        "    dag = DAG(dag_id,\n",
        "              schedule_interval=schedule,\n",
        "              default_args=default_args)\n",
        "\n",
        "    with dag:\n",
        "      tasks_list=list()\n",
        "      for i in range(0, 10):\n",
        "        tasks_list.append(DummyOperator(task_id=f'task_{i}', dag=dag))\n",
        "      tasks_list\n",
        "\n",
        "    return dag\n",
        "\n",
        "\n",
        "# Код для генерации дагов через range()\n",
        "for dag_number in range(0, 5):\n",
        "    number = ''\n",
        "    if dag_number > 0:\n",
        "      number = '_' + str(dag_number)\n",
        "\n",
        "    dag_id = f'dag{number}'\n",
        "\n",
        "    # Настройки по умолчанию\n",
        "    default_args = {'owner': 'airflow',\n",
        "                    'start_date': datetime(2021, 1, 1)\n",
        "                    }\n",
        "\n",
        "    # globals возвращает словарь с глобальной таблицей объектов — словарь текущего модуля.\n",
        "    globals()[dag_id] = create_dag(dag_id, dag_number, default_args)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2nHwpj2ikn7r"
      },
      "source": [
        "Даг нужно написать в файл /root/airflow/dags/dag.py. Проверку можно сделать в веб интерфейсе. Прежде чем даг появится, может пройти ~ 2-3 минут."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
